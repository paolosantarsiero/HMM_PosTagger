{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# IMPLEMENT HIDDEN MARKOW MODEL FOR GREEK AND LATIN POS TAGGER"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from conllu import parse\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "source": [
    "### INIT TAG SETS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_g = ['ADJ','ADP','ADV','CCONJ','DET','INTJ','NOUN','NUM','PART','PRON','PUNCT','SCONJ','VERB','X'];\n",
    "tags_l = ['ADJ','ADP','ADV','AUX','CCONJ','DET','NOUN','NUM','PART','PRON','PROPN','PUNCT','SCONJ','VERB','X'];"
   ]
  },
  {
   "source": [
    "## FUNCTION TO CALCULATE OCCURENCE OF TAG AND WORD\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(word, tag, tagset, count_words, count_tag):\n",
    "    index_of_tag = tagset.index(tag);\n",
    "    totIndex = len(tagset);\n",
    "    # update count of tag\n",
    "    count_tag[index_of_tag] = count_tag[index_of_tag] + 1;\n",
    "    # update count of word\n",
    "    if(word in count_words.keys()):\n",
    "        count_words[word][index_of_tag] = count_words[word][index_of_tag] + 1;\n",
    "    else:\n",
    "        word_row = np.zeros(len(tagset) + 1, dtype=int);\n",
    "        word_row[index_of_tag] = 1;\n",
    "        count_words[word] = word_row;\n",
    "    # update total count of word\n",
    "    count_words[word][totIndex] = count_words[word][totIndex] + 1;\n",
    "    # update total count of tag\n",
    "    count_tag[totIndex] = count_tag[totIndex] + 1;\n",
    "    return count_words, count_tag;"
   ]
  },
  {
   "source": [
    "## CALCULATE EMISSION PROBABILITY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEmissionProbability(word, tag, tagset, count_words, count_tag ,probEmission):\n",
    "    index_of_tag = tagset.index(tag);\n",
    "    # index for total count\n",
    "    totIndex = len(tagset);\n",
    "    #if word exist then update count\n",
    "    if(word in probEmission.keys()):\n",
    "        probEmission[word][index_of_tag] = count_words[word][index_of_tag] / count_tag[index_of_tag];\n",
    "\n",
    "    #if NTOT word exist then create row\n",
    "    else:\n",
    "        prob_row = np.zeros(len(tagset) + 1);\n",
    "        prob_row[index_of_tag] = count_words[word][index_of_tag] / count_tag[index_of_tag];\n",
    "        probEmission[word] = prob_row;\n",
    "    return probEmission;"
   ]
  },
  {
   "source": [
    "## CALCULATE TRANSITION PROBABILITY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateTransictionProbability(word, prev_tag, tag, tagset, transition, count_tag, nSentences, probTransition):\n",
    "    #natural sequence when scan senteces\n",
    "    trans_tag = \"%s_%s\" % (prev_tag,tag);\n",
    "    #Sequence used to saved\n",
    "    trans_tag_real = \"%s_%s\" % (tag,prev_tag);\n",
    "    \n",
    "    if(trans_tag in transition.keys()):\n",
    "        transition[trans_tag] = transition[trans_tag] + 1;\n",
    "    else:\n",
    "        transition[trans_tag] = 1;\n",
    "\n",
    "    #When tag is Q0 (start) the calculate probTransistion with number of sentences\n",
    "    #Else use normal Transition probability\n",
    "    if(trans_tag in probTransition.keys() and prev_tag != 'Q0'):\n",
    "        index_of_tag = tagset.index(prev_tag);\n",
    "        probTransition[trans_tag_real] = transition[trans_tag] /count_tag[index_of_tag];\n",
    "    else:\n",
    "        probTransition[trans_tag_real] = transition[trans_tag]/ nSentences;\n",
    "\n",
    "    return tag, probTransition;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(language,fileType):\n",
    "    nameFile = \"./corpus/%s/data_%s.conllu\" % (language,fileType);\n",
    "    tsv_file = open(nameFile,\"r\",encoding=\"utf-8\").read();\n",
    "    sentences = parse(tsv_file)\n",
    "    return sentences;"
   ]
  },
  {
   "source": [
    "# FUNCTION TO TRAIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(language,statisticsIsEnabled,smoothLemmaIsEnabled):\n",
    "    #select tag sets of language choosen\n",
    "    if language == \"greek\":\n",
    "        tagset = tags_g;\n",
    "    elif language == \"latin\":\n",
    "        tagset = tags_l;\n",
    "    else:\n",
    "        raise Exception(\"Language not found!\");\n",
    "\n",
    "    # INIT DATA STRUCTURE\n",
    "    count_words = dict();\n",
    "    count_tag = np.zeros(len(tagset) + 1, dtype = int);\n",
    "    probEmission = dict();\n",
    "    probTransition = dict();\n",
    "    transition = dict();\n",
    "    statistics = np.zeros(len(tagset));\n",
    "\n",
    "    # DATAS FOR USE LEMMA\n",
    "    count_words_lemmas = dict();\n",
    "    count_tag_lemmas = np.zeros(len(tagset) + 1, dtype = int);\n",
    "    probEmissionLemmas = dict();\n",
    "\n",
    "    #load statistics\n",
    "    if statisticsIsEnabled == True:\n",
    "        statistics = calculateStatisticPosTagging(language)\n",
    "    \n",
    "    #Count number of sentence for calculate probTransistio with tag start Q0\n",
    "    nSentences = 0;\n",
    "\n",
    "    sentences = readFile(language,\"train\")\n",
    "    for sentence in sentences:\n",
    "        prev_tag = 'Q0';\n",
    "        nSentences = nSentences + 1;\n",
    "        for token in sentence:\n",
    "            word = token[\"form\"];\n",
    "            tag = token[\"upos\"];\n",
    "            count_words, count_tag = count(word, tag, tagset, count_words, count_tag);\n",
    "            probEmission = calculateEmissionProbability(word,tag,tagset, count_words, count_tag, probEmission);\n",
    "            if smoothLemmaIsEnabled == True:\n",
    "                lemma = token[\"lemma\"]\n",
    "                count_words_lemmas, count_tag_lemmas = count(lemma, tag, tagset, count_words_lemmas, count_tag_lemmas);\n",
    "                probEmissionLemmas = calculateEmissionProbability(lemma,tag,tagset, count_words_lemmas, count_tag_lemmas, probEmissionLemmas);\n",
    "\n",
    "            prev_tag, probTransition = calculateTransictionProbability(word, prev_tag, tag, tagset, transition, count_tag, nSentences, probTransition)\n",
    "    return sentences,tagset, probEmission, probTransition,statistics,probEmissionLemmas;"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    words = [];\n",
    "    for token in sentence:\n",
    "        words.append(token[\"form\"]);\n",
    "    return words"
   ]
  },
  {
   "source": [
    "### CALCULATE STATISTIC POS TAGGIN (SMOOTHING)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateStatisticPosTagging(language):\n",
    "    #select tag sets of language choosen\n",
    "    if language == \"greek\":\n",
    "        tagset = tags_g;\n",
    "    elif language == \"latin\":\n",
    "        tagset = tags_l;\n",
    "    else:\n",
    "        raise Exception(\"Language not found!\");\n",
    "\n",
    "    count_words = dict();\n",
    "    count_tag = np.zeros(len(tagset) + 1, dtype = int);\n",
    "    count_tag_one_occured = np.zeros(len(tagset), dtype = int);\n",
    "    count_tag_one_occured_total = 0;\n",
    "\n",
    "    statistics = np.zeros(len(tagset));\n",
    "    \n",
    "\n",
    "    sentences = readFile(language,\"dev\");\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            word = token[\"form\"];\n",
    "            tag = token[\"upos\"];\n",
    "            count_words, count_tag = count(word, tag, tagset, count_words, count_tag);\n",
    "\n",
    "    totIndex = len(tagset);\n",
    "    for word in count_words:\n",
    "        if count_words[word][totIndex] == 1:\n",
    "            index_of_tag = np.argmax(count_words[word]);\n",
    "            count_tag_one_occured[index_of_tag] = count_tag_one_occured[index_of_tag] + 1;\n",
    "            count_tag_one_occured_total =  count_tag_one_occured_total + 1;\n",
    "\n",
    "    for tag in tagset:\n",
    "        index_of_tag = tagset.index(tag);\n",
    "        statistics[index_of_tag] = count_tag_one_occured[index_of_tag] / count_tag_one_occured_total;\n",
    "    return statistics;"
   ]
  },
  {
   "source": [
    "## IMPLEMENT SMOOTHING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def selectSmoothing(type, probEmission, word, index_of_tag, tagset, statistics, probEmissionLemma):\n",
    "    # NO SMOOTHING\n",
    "    if word in probEmission.keys():\n",
    "       return probEmission[word][index_of_tag]\n",
    "    elif type == 0 and word not in probEmission.keys():\n",
    "        return 0.00001;\n",
    "    # IF WORD NOT EXIST THEN SET NOUN PROBABILITY 1\n",
    "    elif type == 1 and word not in probEmission.keys():\n",
    "        prob_row = np.zeros(len(tagset) + 1);\n",
    "        index_of_noun = tagset.index(\"NOUN\");\n",
    "        prob_row[index_of_noun] = 1;\n",
    "        return prob_row[index_of_tag];\n",
    "    # IF WORD NOT EXIST THEN SET NOUN AND VERB EQUI PROBABILITY (0.5)\n",
    "    elif type == 2 and word not in probEmission.keys():\n",
    "        prob_row = np.zeros(len(tagset) + 1);\n",
    "        index_of_noun = tagset.index(\"NOUN\");\n",
    "        index_of_verb = tagset.index(\"VERB\");\n",
    "        prob_row[index_of_noun] = 0.5;\n",
    "        prob_row[index_of_verb] = 0.5;\n",
    "        return prob_row[index_of_tag];\n",
    "    # IF WORD NOT EXIST THEN EVERY TAG HAS PROBABILITY 1/#(pos_tags)\n",
    "    elif type == 3 and word not in probEmission.keys():\n",
    "        unk_prob = 1 / len(tagset);\n",
    "        prob_row = np.full(len(tagset) + 1, unk_prob);\n",
    "        return prob_row[index_of_tag];\n",
    "    # IF WORD NOT EXIST THEN EVERY TAG HAS POS TAGGER STATISTICS FOR EVERY WORD IN DEV THAT HAS 1 OCCURENCE\n",
    "    elif type == 4 and word not in probEmission.keys():\n",
    "        return statistics[index_of_tag];\n",
    "    elif type == 5 and word in probEmissionLemma.keys():\n",
    "        word = ntlk.lemmatize(word);\n",
    "        return probEmissionLemma[word][index_of_tag]\n",
    "    else:\n",
    "        return 0.00001;"
   ]
  },
  {
   "source": [
    "# DECODING WITH VITERBI"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, tagset, probEmission, probTransition, smoothingType, statistics, probEmissionLemmas):\n",
    "    words = tokenize_sentence(sentence);\n",
    "    start_tag = \"Q0\";\n",
    "    viterbi_matrix = np.zeros((len(tagset),len(words)));\n",
    "    backtrace = np.zeros(len(words), dtype = int);\n",
    "    probabilites = np.zeros(len(words));\n",
    "    t = 0;\n",
    "   \n",
    "    for word in words:\n",
    "        # Calculate viterbi column for every tag possible\n",
    "        for tag in tagset:\n",
    "            index_of_tag = tagset.index(tag);\n",
    "            #Get Emission probabilty of word ( HERE WHEN CAN APPLY SMOOTHING)\n",
    "            probE = selectSmoothing(smoothingType, probEmission, word, index_of_tag, tagset, statistics, probEmissionLemmas);\n",
    "\n",
    "            if probE == 0:\n",
    "                probE = np.log(0.00001)\n",
    "            else:\n",
    "                probE = np.log(probE)\n",
    "\n",
    "            #Run first iteration of viterbi to initialize first column\n",
    "            if t == 0:\n",
    "                tran_tag = \"%s_%s\" % (tag,start_tag);   \n",
    "                probT = np.log(0.00001);\n",
    "                if tran_tag in probTransition.keys():\n",
    "                    probT = np.log(probTransition[tran_tag]);\n",
    "                viterbi_matrix[index_of_tag][t] = probE + probT;\n",
    "            else:\n",
    "                max_tmp = np.zeros(len(tagset));\n",
    "                for i in range(0,len(tagset)):\n",
    "                    tran_tag = \"%s_%s\" % (tag,tagset[i]);\n",
    "                    probT = np.log(0.00001);\n",
    "                    if tran_tag in probTransition.keys():\n",
    "                        probT = np.log(probTransition[tran_tag]);\n",
    "\n",
    "                    max_tmp[i] = viterbi_matrix[i,t-1] + probT;\n",
    "                viterbi_matrix[index_of_tag,t] = np.amax(max_tmp) + probE;\n",
    "\n",
    "        index_max_values = np.argmax(viterbi_matrix[:,t]);  \n",
    "        backtrace[t] = index_max_values;\n",
    "        probabilites[t] = viterbi_matrix[index_max_values,t];\n",
    "        t= t +1;\n",
    "    return backtrace,probabilites;"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printPosTag(sentence, tagset, backtrace , probabilities):\n",
    "    i = 0;\n",
    "    words = tokenize_sentence(sentence);\n",
    "    for word in words:\n",
    "        print(\"WORD_ROW -> \" + word + \" \" + tagset[backtrace[i]] + \"     prob -> \" + str(probabilities[i]))\n",
    "        i = i + 1;"
   ]
  },
  {
   "source": [
    "## TRAIN AND DECODING GREEK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SMOOTHING_TYPE = 0;\n",
    "LOAD_STATISTIC_POS = False;\n",
    "LOAD_EMISSION_PROB_LEMMAS = False;\n",
    "\n",
    "sentences_greek, tagset_greek, probEmission_greek, probTransition_greek, statistics_greek, probEmissionLemmas_greek  =  train(\"greek\", LOAD_STATISTIC_POS, LOAD_EMISSION_PROB_LEMMAS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================== TEST ON GREEK ================================== \nWORD_ROW -> ἐρᾷ VERB     prob -> -9.268481272273874\nWORD_ROW -> μὲν PART     prob -> -15.012261505759101\nWORD_ROW -> ἁγνὸς ADJ     prob -> -25.780646354223645\nWORD_ROW -> οὐρανὸς NOUN     prob -> -35.304979691779565\nWORD_ROW -> τρῶσαι VERB     prob -> -37.33968533961801\nWORD_ROW -> χθόνα NOUN     prob -> -46.42668418324466\nWORD_ROW -> , PUNCT     prob -> -48.80483279303211\nWORD_ROW -> ἔρως NOUN     prob -> -59.96014696849298\nWORD_ROW -> δὲ PART     prob -> -64.59106304839658\nWORD_ROW -> γαῖαν NOUN     prob -> -72.5121179919485\nWORD_ROW -> λαμβάνει VERB     prob -> -74.95228874789511\nWORD_ROW -> γάμου NOUN     prob -> -85.04102125627635\nWORD_ROW -> τυχεῖν VERB     prob -> -94.40983865023648\nWORD_ROW -> · PUNCT     prob -> -97.06068988639385\n"
     ]
    }
   ],
   "source": [
    "print(\"==================== TEST ON GREEK ================================== \")\n",
    "backtrace_greek, probabilities_greek = viterbi(sentences_greek[0], tagset_greek, probEmission_greek, probTransition_greek, SMOOTHING_TYPE, statistics_greek, probEmissionLemmas_greek);\n",
    "printPosTag(sentences_greek[0], tagset_greek, backtrace_greek, probabilities_greek);"
   ]
  },
  {
   "source": [
    "## TRAIN AND DECODING LATIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences_latin, tagset_latin, probEmission_latin, probTransition_latin, statistics_latin,probEmissionLemmas_latin = train(\"latin\",LOAD_STATISTIC_POS, LOAD_EMISSION_PROB_LEMMAS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==================== TEST ON LATIN ================================== \nWORD_ROW -> + PUNCT     prob -> -3.0409226340125732\nWORD_ROW -> In ADP     prob -> -9.439532515808455\nWORD_ROW -> Dei PROPN     prob -> -14.211431251275023\nWORD_ROW -> nomine NOUN     prob -> -19.347702756413725\nWORD_ROW -> regnante VERB     prob -> -25.41676081668182\nWORD_ROW -> domno NOUN     prob -> -32.09007095933274\nWORD_ROW -> nostro DET     prob -> -37.32125225570135\nWORD_ROW -> Carulo PROPN     prob -> -44.352569396270525\nWORD_ROW -> rege NOUN     prob -> -50.94443448474381\nWORD_ROW -> Francorum NOUN     prob -> -59.12142501243105\nWORD_ROW -> et CCONJ     prob -> -62.08688424838046\nWORD_ROW -> Langobardorum NOUN     prob -> -69.5169467052356\nWORD_ROW -> , PUNCT     prob -> -71.71924027267977\nWORD_ROW -> anno NOUN     prob -> -77.47089829042083\nWORD_ROW -> regni NOUN     prob -> -85.09864464321117\nWORD_ROW -> eius DET     prob -> -89.89233065797255\nWORD_ROW -> quo PRON     prob -> -98.0120052842971\nWORD_ROW -> coepit VERB     prob -> -103.88455098746397\nWORD_ROW -> Langobardiam PROPN     prob -> -111.6989517354119\nWORD_ROW -> primo ADJ     prob -> -120.31805436877386\nWORD_ROW -> , PUNCT     prob -> -122.1806078002921\nWORD_ROW -> septimo ADJ     prob -> -130.6077480658951\nWORD_ROW -> decimo ADJ     prob -> -137.10431615708202\nWORD_ROW -> kalendas NOUN     prob -> -143.630340290092\nWORD_ROW -> augustas ADJ     prob -> -152.13002485574665\nWORD_ROW -> , PUNCT     prob -> -153.9925782872649\nWORD_ROW -> per ADP     prob -> -158.88516417473824\nWORD_ROW -> indictionem NOUN     prob -> -164.9927314913198\nWORD_ROW -> duodecimam ADJ     prob -> -172.91715827355114\nWORD_ROW -> . PUNCT     prob -> -175.7909476589246\n"
     ]
    }
   ],
   "source": [
    "print(\"==================== TEST ON LATIN ================================== \")\n",
    "backtrace_latin, probabilities_latin = viterbi(sentences_latin[0], tagset_latin, probEmission_latin, probTransition_latin, SMOOTHING_TYPE, statistics_latin,probEmissionLemmas_latin);\n",
    "printPosTag(sentences_latin[0], tagset_latin, backtrace_latin, probabilities_latin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy(language, tagset, probEmission, probTransition, smoothingType, statistics):\n",
    "    probEmissionLemmas = dict();\n",
    "    accuracy = 0;\n",
    "    sentences_test = readFile(language,\"test\");\n",
    "    count_tag_correct = 0;\n",
    "    count_total_tag = 0;\n",
    "    errors = dict();\n",
    "    for sentence in sentences_test:\n",
    "        backtrace_test, probabilities_test = viterbi(sentence, tagset, probEmission, probTransition, smoothingType, statistics, probEmissionLemmas);\n",
    "         #Get real tag\n",
    "        i = 0;\n",
    "        for token in sentence:\n",
    "            real_tag = token[\"upos\"];\n",
    "            if tagset.index(real_tag) == backtrace_test[i]:\n",
    "                count_tag_correct = count_tag_correct + 1;\n",
    "            else:\n",
    "                errors[token[\"form\"]] = [real_tag,tagset[backtrace_test[i]]]\n",
    "            count_total_tag = count_total_tag + 1;\n",
    "            i = i + 1;\n",
    "    accuracy = count_tag_correct/count_total_tag\n",
    "    return accuracy,errors;"
   ]
  },
  {
   "source": [
    "### CALCULATE ACCURACY ON TEST SET OF GREEK"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACCURACY GREEK  0.729090128345818\n"
     ]
    }
   ],
   "source": [
    "accuracy_greek,errors_greek = calculateAccuracy(\"greek\", tagset_greek, probEmission_greek, probTransition_greek, SMOOTHING_TYPE, statistics_greek);\n",
    "print(\"ACCURACY GREEK  \" + str(accuracy_greek))\n"
   ]
  },
  {
   "source": [
    "### CALCULATE ACCURACY ON TEST SET OF LATIN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACCURACY LATIN 0.9552722289131609\n"
     ]
    }
   ],
   "source": [
    "accuracy_latin,errors_latin = calculateAccuracy(\"latin\", tagset_latin, probEmission_latin, probTransition_latin, SMOOTHING_TYPE, statistics_latin);\n",
    "print(\"ACCURACY LATIN \" + str(accuracy_latin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizeErrors(errors,tagset):\n",
    "    probabilites_error = dict();\n",
    "    for word in errors:\n",
    "        real_tag = errors[word][0]\n",
    "        estimate_tag = errors[word][1]\n",
    "        index_estimated_tag = tagset.index(estimate_tag);\n",
    "        if real_tag in probabilites_error.keys():\n",
    "            probabilites_error[real_tag][index_estimated_tag] = probabilites_error[real_tag][index_estimated_tag] + 1;\n",
    "        else:\n",
    "            prob_error = np.zeros(len(tagset), dtype = int);\n",
    "            prob_error[index_estimated_tag] = 1;\n",
    "            probabilites_error[real_tag] = prob_error;\n",
    "    return probabilites_error"
   ]
  },
  {
   "source": [
    "### Analize erros"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'VERB': array([  1,   0, 123,   0,   0,   0, 413,   0, 124,   0, 296,   0,   0,\n",
       "          0]),\n",
       " 'ADV': array([ 3,  4,  0, 11,  0,  0, 10,  0, 37,  2, 17,  3, 44,  0]),\n",
       " 'PRON': array([99,  0,  7,  0,  0,  0, 13,  0,  2,  0,  6,  0, 11,  1]),\n",
       " 'NOUN': array([ 33,   1, 120,   0,   0,   0,   0,   0,  65,   1, 240,   0, 549,\n",
       "          0]),\n",
       " 'ADJ': array([  0,   0,  33,   0,   0,   0, 329,  13,  30,  25, 131,   0, 238,\n",
       "          0]),\n",
       " 'DET': array([ 0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,  1,  1]),\n",
       " 'ADP': array([0, 0, 5, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 0]),\n",
       " 'SCONJ': array([ 0,  1, 11,  1,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0]),\n",
       " 'NUM': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'CCONJ': array([0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]),\n",
       " 'PUNCT': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0]),\n",
       " 'INTJ': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0]),\n",
       " 'X': array([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])}"
      ]
     },
     "metadata": {},
     "execution_count": 760
    }
   ],
   "source": [
    "analizeErrors(errors_greek,tagset_greek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'VERB': array([ 0,  2,  0,  7,  0,  0, 47,  0,  0,  0,  5, 40,  0,  0,  0]),\n",
       " 'PROPN': array([  5,   0,   0,   0,   0,   0, 120,   0,   0,   0,   0, 107,   0,\n",
       "          8,   1]),\n",
       " 'ADV': array([0, 1, 0, 0, 4, 0, 3, 1, 0, 0, 0, 1, 1, 0, 0]),\n",
       " 'DET': array([0, 2, 0, 0, 0, 0, 4, 0, 0, 6, 2, 0, 0, 0, 0]),\n",
       " 'NOUN': array([ 7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4, 17,  0,  7,  0]),\n",
       " 'ADP': array([1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0]),\n",
       " 'CCONJ': array([0, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 0]),\n",
       " 'ADJ': array([ 0,  1,  0,  0,  0,  0, 27,  0,  0,  0,  1, 15,  0,  2,  0]),\n",
       " 'NUM': array([0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0]),\n",
       " 'AUX': array([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 4, 0]),\n",
       " 'SCONJ': array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]),\n",
       " 'PRON': array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "metadata": {},
     "execution_count": 761
    }
   ],
   "source": [
    "analizeErrors(errors_latin,tagset_latin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}